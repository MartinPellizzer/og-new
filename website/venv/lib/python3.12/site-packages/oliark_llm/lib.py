from llama_cpp import Llama

llm = None
llm_loaded = False

vault_tmp = f'/home/ubuntu/vault-tmp'
model_path_backup = f'{vault_tmp}/llms/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf'

def llm_reply(prompt, model_path=model_path_backup, n_ctx=8196, max_tokens=4096, temperature=0.7):
    global llm
    global llm_loaded
    if llm_loaded == False:
        llm = Llama(
            model_path = model_path,
            n_gpu_layers = -1,
            n_ctx = n_ctx,
        )
        llm_loaded = True
    stream = llm.create_chat_completion(
        messages = [{
            'role': 'user',
            'content': prompt,
        }],
        stream = True,
        temperature = temperature,
        max_tokens = max_tokens,
    )
    reply = ''
    for chunk in stream:
        if 'content' in chunk['choices'][0]['delta'].keys():
            token = chunk['choices'][0]['delta']['content']
            reply += token
            print(token, end='', flush=True)
    return reply

